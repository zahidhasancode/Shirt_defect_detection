{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ie4x6pyF8Ix-",
        "outputId": "b6758edf-3c4c-4d70-8bc4-478d91b45f05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 130 images belonging to 2 classes.\n",
            "Epoch 1/60\n",
            "5/5 [==============================] - 16s 355ms/step - loss: 0.8388 - accuracy: 0.5154\n",
            "Epoch 2/60\n",
            "5/5 [==============================] - 4s 674ms/step - loss: 0.6889 - accuracy: 0.5538\n",
            "Epoch 3/60\n",
            "5/5 [==============================] - 4s 1s/step - loss: 0.6594 - accuracy: 0.6462\n",
            "Epoch 4/60\n",
            "5/5 [==============================] - 3s 845ms/step - loss: 0.6871 - accuracy: 0.5769\n",
            "Epoch 5/60\n",
            "5/5 [==============================] - 4s 665ms/step - loss: 0.5847 - accuracy: 0.6769\n",
            "Epoch 6/60\n",
            "5/5 [==============================] - 4s 1s/step - loss: 0.6022 - accuracy: 0.6308\n",
            "Epoch 7/60\n",
            "5/5 [==============================] - 3s 666ms/step - loss: 0.6068 - accuracy: 0.6077\n",
            "Epoch 8/60\n",
            "5/5 [==============================] - 4s 670ms/step - loss: 0.5621 - accuracy: 0.7000\n",
            "Epoch 9/60\n",
            "5/5 [==============================] - 4s 1s/step - loss: 0.3634 - accuracy: 0.9154\n",
            "Epoch 10/60\n",
            "5/5 [==============================] - 4s 676ms/step - loss: 0.2560 - accuracy: 0.9308\n",
            "Epoch 11/60\n",
            "5/5 [==============================] - 4s 862ms/step - loss: 0.8005 - accuracy: 0.7462\n",
            "Epoch 12/60\n",
            "5/5 [==============================] - 4s 1s/step - loss: 0.4587 - accuracy: 0.7538\n",
            "Epoch 13/60\n",
            "5/5 [==============================] - 4s 677ms/step - loss: 0.4035 - accuracy: 0.8462\n",
            "Epoch 14/60\n",
            "5/5 [==============================] - 4s 661ms/step - loss: 0.4396 - accuracy: 0.8538\n",
            "Epoch 15/60\n",
            "5/5 [==============================] - 4s 883ms/step - loss: 0.3339 - accuracy: 0.9154\n",
            "Epoch 16/60\n",
            "5/5 [==============================] - 4s 871ms/step - loss: 0.2227 - accuracy: 0.9462\n",
            "Epoch 17/60\n",
            "5/5 [==============================] - 4s 670ms/step - loss: 0.1306 - accuracy: 0.9462\n",
            "Epoch 18/60\n",
            "5/5 [==============================] - 4s 811ms/step - loss: 0.1233 - accuracy: 0.9692\n",
            "Epoch 19/60\n",
            "5/5 [==============================] - 4s 678ms/step - loss: 0.8915 - accuracy: 0.7308\n",
            "Epoch 20/60\n",
            "5/5 [==============================] - 4s 673ms/step - loss: 0.2754 - accuracy: 0.9154\n",
            "Epoch 21/60\n",
            "5/5 [==============================] - 4s 668ms/step - loss: 0.3316 - accuracy: 0.9154\n",
            "Epoch 22/60\n",
            "5/5 [==============================] - 3s 647ms/step - loss: 0.2806 - accuracy: 0.9462\n",
            "Epoch 23/60\n",
            "5/5 [==============================] - 3s 651ms/step - loss: 0.2394 - accuracy: 0.9000\n",
            "Epoch 24/60\n",
            "5/5 [==============================] - 4s 776ms/step - loss: 0.1553 - accuracy: 0.9462\n",
            "Epoch 25/60\n",
            "5/5 [==============================] - 4s 701ms/step - loss: 0.0820 - accuracy: 0.9769\n",
            "Epoch 26/60\n",
            "5/5 [==============================] - 4s 672ms/step - loss: 0.0835 - accuracy: 0.9769\n",
            "Epoch 27/60\n",
            "5/5 [==============================] - 4s 772ms/step - loss: 0.1199 - accuracy: 0.9462\n",
            "Epoch 28/60\n",
            "5/5 [==============================] - 4s 872ms/step - loss: 0.7882 - accuracy: 0.8385\n",
            "Epoch 29/60\n",
            "5/5 [==============================] - 4s 864ms/step - loss: 0.4309 - accuracy: 0.8615\n",
            "Epoch 30/60\n",
            "5/5 [==============================] - 4s 786ms/step - loss: 0.2247 - accuracy: 0.9231\n",
            "Epoch 31/60\n",
            "5/5 [==============================] - 4s 663ms/step - loss: 0.1713 - accuracy: 0.9692\n",
            "Epoch 32/60\n",
            "5/5 [==============================] - 3s 649ms/step - loss: 0.1784 - accuracy: 0.9385\n",
            "Epoch 33/60\n",
            "5/5 [==============================] - 4s 816ms/step - loss: 0.1267 - accuracy: 0.9692\n",
            "Epoch 34/60\n",
            "5/5 [==============================] - 3s 655ms/step - loss: 0.1042 - accuracy: 0.9692\n",
            "Epoch 35/60\n",
            "5/5 [==============================] - 4s 684ms/step - loss: 0.1215 - accuracy: 0.9538\n",
            "Epoch 36/60\n",
            "5/5 [==============================] - 4s 730ms/step - loss: 0.0809 - accuracy: 0.9615\n",
            "Epoch 37/60\n",
            "5/5 [==============================] - 4s 650ms/step - loss: 0.1207 - accuracy: 0.9692\n",
            "Epoch 38/60\n",
            "5/5 [==============================] - 4s 857ms/step - loss: 0.1152 - accuracy: 0.9692\n",
            "Epoch 39/60\n",
            "5/5 [==============================] - 4s 783ms/step - loss: 0.1081 - accuracy: 0.9615\n",
            "Epoch 40/60\n",
            "5/5 [==============================] - 4s 636ms/step - loss: 0.1309 - accuracy: 0.9692\n",
            "Epoch 41/60\n",
            "5/5 [==============================] - 3s 833ms/step - loss: 0.0389 - accuracy: 0.9846\n",
            "Epoch 42/60\n",
            "5/5 [==============================] - 4s 794ms/step - loss: 0.0644 - accuracy: 0.9692\n",
            "Epoch 43/60\n",
            "5/5 [==============================] - 3s 643ms/step - loss: 0.1124 - accuracy: 0.9692\n",
            "Epoch 44/60\n",
            "5/5 [==============================] - 4s 653ms/step - loss: 0.0481 - accuracy: 0.9923\n",
            "Epoch 45/60\n",
            "5/5 [==============================] - 4s 862ms/step - loss: 0.1217 - accuracy: 0.9692\n",
            "Epoch 46/60\n",
            "5/5 [==============================] - 3s 658ms/step - loss: 0.0933 - accuracy: 0.9846\n",
            "Epoch 47/60\n",
            "5/5 [==============================] - 4s 867ms/step - loss: 0.0400 - accuracy: 0.9923\n",
            "Epoch 48/60\n",
            "5/5 [==============================] - 4s 937ms/step - loss: 0.0660 - accuracy: 0.9769\n",
            "Epoch 49/60\n",
            "5/5 [==============================] - 4s 654ms/step - loss: 0.0467 - accuracy: 0.9846\n",
            "Epoch 50/60\n",
            "5/5 [==============================] - 4s 860ms/step - loss: 0.0583 - accuracy: 0.9769\n",
            "Epoch 51/60\n",
            "5/5 [==============================] - 4s 644ms/step - loss: 0.0492 - accuracy: 0.9923\n",
            "Epoch 52/60\n",
            "5/5 [==============================] - 3s 643ms/step - loss: 0.0624 - accuracy: 0.9846\n",
            "Epoch 53/60\n",
            "5/5 [==============================] - 4s 858ms/step - loss: 0.0184 - accuracy: 0.9923\n",
            "Epoch 54/60\n",
            "5/5 [==============================] - 3s 662ms/step - loss: 0.0246 - accuracy: 0.9846\n",
            "Epoch 55/60\n",
            "5/5 [==============================] - 4s 642ms/step - loss: 0.0729 - accuracy: 0.9692\n",
            "Epoch 56/60\n",
            "5/5 [==============================] - 4s 876ms/step - loss: 0.0308 - accuracy: 0.9846\n",
            "Epoch 57/60\n",
            "5/5 [==============================] - 3s 850ms/step - loss: 0.0281 - accuracy: 0.9923\n",
            "Epoch 58/60\n",
            "5/5 [==============================] - 3s 670ms/step - loss: 0.0532 - accuracy: 0.9846\n",
            "Epoch 59/60\n",
            "5/5 [==============================] - 4s 1s/step - loss: 0.0164 - accuracy: 1.0000\n",
            "Epoch 60/60\n",
            "5/5 [==============================] - 4s 675ms/step - loss: 0.0701 - accuracy: 0.9692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Define constants\n",
        "IMAGE_SIZE = (150, 150)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 60\n",
        "\n",
        "# Create an ImageDataGenerator for data augmentation and preprocessing\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "# Load and prepare the training dataset\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/content/unzip1/shirt_resize_img',\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary')\n",
        "\n",
        "# Build the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=1)\n",
        "\n",
        "# # Save the trained model\n",
        "model.save('new1_shirt_defect_model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Specify the zip file you want to unzip and the directory to extract it to\n",
        "zip_file_name = 'shirt_resize_img.zip'\n",
        "extracted_dir = '/content/unzip1'\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile(zip_file_name, 'r') as zipf:\n",
        "    zipf.extractall(extracted_dir)\n"
      ],
      "metadata": {
        "id": "uCyNqoGy_Q4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "# Load your pre-trained model\n",
        "model = load_model('new1_shirt_defect_model.h5')\n",
        "model.make_predict_function()\n",
        "\n",
        "# Function to preprocess the uploaded image\n",
        "def preprocess_image(image_path):\n",
        "    img = image.load_img(image_path, target_size=(150, 150))\n",
        "    img = image.img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = img / 255.0  # Normalize the image\n",
        "    return img\n",
        "\n",
        "# Function to classify the image\n",
        "def classify_shirt(image_path):\n",
        "    # Preprocess the uploaded image\n",
        "    img = preprocess_image(image_path)\n",
        "\n",
        "    # Make a prediction\n",
        "    prediction = model.predict(img)\n",
        "\n",
        "    # Determine the result\n",
        "    if prediction[0][0] > 0.5:\n",
        "        return 'Defected Shirt'\n",
        "    else:\n",
        "        return 'Good Shirt'\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    image_path = input(\"Enter the path to the image you want to classify: \")\n",
        "    result = classify_shirt(image_path)\n",
        "    print(\"Classification Result:\", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XlSZlukikk5",
        "outputId": "1edc12c0-8900-4ed5-8194-e701b72facdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the path to the image you want to classify: /content/unzip1/shirt_resize_img/Good_shirt/20230905_002509.jpg\n",
            "1/1 [==============================] - 0s 231ms/step\n",
            "Classification Result: Good Shirt\n"
          ]
        }
      ]
    }
  ]
}